---
description: Generate daily experiment status report for Slack. Triggered by CI or manually.
---

# Daily Experiment Status Report

Generate a status report of the current user's experiments, analyze failures, and create an actionable TODO list.

## Prerequisites

You have access to:
- `beaker` CLI (authenticated via BEAKER_TOKEN)
- `gh` CLI (authenticated via GH_TOKEN)
- `curl` for Slack webhook

## Reference

See `check-beaker-job-status.mdc` for detailed Beaker RPC API documentation.

## Step 1: Get Recent Experiments

**Default:** Last 24 hours. If user specifies a different time period (e.g., "last 3 days", "since Monday"), adjust accordingly.

Use the Beaker RPC API for precise server-side filtering:

```bash
# Get current user's author ID
AUTHOR_ID=$(beaker account whoami --format json | jq -r '.[0].id')

# Calculate timestamp for 24 hours ago (or user-specified window)
SINCE=$(date -u -v-24H +"%Y-%m-%dT%H:%M:%SZ")  # macOS
# SINCE=$(date -u -d "24 hours ago" +"%Y-%m-%dT%H:%M:%SZ")  # Linux

# Get current user's experiments
beaker rpc call ListWorkloads '{
  "options": {
    "authorId": "'"$AUTHOR_ID"'",
    "organizationId": "us_wvnghctl47k0",
    "workloadType": "WORKLOAD_TYPE_EXPERIMENT",
    "createdAfter": "'"$SINCE"'",
    "pageSize": 100
  }
}'
```

From the response, filter to experiments with status `running`, `failed`, or `queued`.

**Sort by most recent first** (by created timestamp, descending).

## Step 2: Group Related Experiments

Experiments often share a common prefix indicating they're part of the same group:
- `base_spectral-train-*` â†’ "base_spectral" group
- `tokenization_v2-train-*` â†’ "tokenization_v2" group
- `ablation_patch_size-*` â†’ "ablation_patch_size" group

Group by extracting the prefix before `-train-` or the first UUID-like suffix.

## Step 3: Analyze Each Group

For each experiment group:

### Running Experiments
```bash
beaker experiment get <EXP_ID> --format json
```
Report:
- How long running
- Progress if available (check description field for W&B info)
- Any warnings in recent logs

### Failed Experiments
Use patterns from `debug-beaker-experiments.mdc`:
```bash
beaker experiment logs <EXP_ID> 2>&1 | tail -300
```

Identify error type:
| Error Pattern | Category | Action |
|--------------|----------|--------|
| `CUDA out of memory` | OOM | Suggest reduce batch_size or gradient accumulation |
| `EinopsError: Shape mismatch` | Shape | Find the file/line, check recent changes |
| `ModuleNotFoundError` | Import | Check if dep missing from pyproject.toml |
| `KeyError` in config | Config | Check config file for typo |
| `NCCL timeout` | Multi-GPU | Likely transient, suggest retry |
| Preempted | Infra | Just note it, will auto-retry |

## Step 4: Find Related PRs

For each failure, search for existing PRs that might fix it:
```bash
gh pr list --state open --search "<error_keyword> OR <affected_file>"
gh pr list --state merged --search "<error_keyword>" --limit 5
```

If a recent merged PR might have caused the issue:
```bash
gh pr list --state merged --limit 10
```

## Step 5: Generate Report

Format Slack-friendly report (write to `report.txt`), making experiment names clickable Beaker links.

If `REPORT_USER` env var is set, include it in the report header.

```
ğŸ“Š *Experiment Status Report* - {date} - {username}

*Summary:* {N} running | {M} queued | {P} failed | {K} groups

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”„ *RUNNING* (most recent first)

*{group_name}* ({N} experiments)
â€¢ `exp_id_1` - started 1h ago, step 8k/100k
â€¢ `exp_id_2` - started 2h ago, step 15k/100k

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â³ *QUEUED* ({N} experiments)

â€¢ `exp_id_1` - queued 30m ago
â€¢ `exp_id_2` - queued 1h ago

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âŒ *FAILED*

*{group_name}* ({N} experiments, same error)
â€¢ Error: `RuntimeError: CUDA out of memory`
â€¢ Affected: `exp_id_1`, `exp_id_2`, `exp_id_3`
â€¢ Cause: Batch size 64 too large for 40GB A100
â€¢ Fix: Reduce to batch_size=32 or add gradient_accumulation_steps=2
â€¢ Related PR: <https://github.com/allenai/helios/pull/123|#123 Fix OOM in large models>

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‹ *ACTION ITEMS*

1. [ ] Rerun `base_spectral` group with batch_size=32
2. [ ] Review PR #123 for OOM fix, merge if ready
3. [ ] Investigate shape mismatch in `tokenization_v2` - check `nn/encoder.py:142`
4. [ ] `ablation_patch_size` experiments look healthy, just monitor

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

## Step 6: Send to Slack

```bash
curl -X POST "$SLACK_WEBHOOK_URL" \
  -H "Content-Type: application/json" \
  -d "$(jq -n --arg text "$(cat report.txt)" '{text: $text}')"
```

## Notes

- Keep the report concise - group similar failures together
- Prioritize actionable items in the TODO list
- If an experiment group has mixed results, note what's different
- Link to W&B runs if URLs are in the experiment description
